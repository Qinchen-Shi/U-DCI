{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from util import load_data, load_graph\n",
    "from models.udci import U_DCI\n",
    "from models.clf_model import Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the edge_index done!\n",
      "Ratio of fraudsters:  0.026376564969004496\n",
      "Number of edges:  18257\n",
      "Number of users:  8227\n",
      "Number of objects:  1000\n",
      "Number of nodes:  9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:11<?, ?it/s]\n",
      "Epoch: 20/50 | Current loss: 0.389:  40%|████      | 20/50 [23:28<44:28, 88.95s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jz22185\\OneDrive - University of Bristol\\DCI-Fintech\\U_DCI.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jz22185/OneDrive%20-%20University%20of%20Bristol/DCI-Fintech/U_DCI.ipynb#ch0000002?line=121'>122</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, args\u001b[39m.\u001b[39mepochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jz22185/OneDrive%20-%20University%20of%20Bristol/DCI-Fintech/U_DCI.ipynb#ch0000002?line=122'>123</a>\u001b[0m     model_pretrain\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jz22185/OneDrive%20-%20University%20of%20Bristol/DCI-Fintech/U_DCI.ipynb#ch0000002?line=123'>124</a>\u001b[0m     loss_pretrain \u001b[39m=\u001b[39m model_pretrain(feats, shuf_feats, adj_1hop, adj_2hop, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, cluster_info, args\u001b[39m.\u001b[39;49mnum_cluster)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jz22185/OneDrive%20-%20University%20of%20Bristol/DCI-Fintech/U_DCI.ipynb#ch0000002?line=124'>125</a>\u001b[0m     \u001b[39mif\u001b[39;00m optimizer_train \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jz22185/OneDrive%20-%20University%20of%20Bristol/DCI-Fintech/U_DCI.ipynb#ch0000002?line=125'>126</a>\u001b[0m         optimizer_train\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jz22185\\OneDrive - University of Bristol\\DCI-Fintech\\models\\udci.py:19\u001b[0m, in \u001b[0;36mU_DCI.forward\u001b[1;34m(self, seq1, seq2, sadj, sadj2, msk, samp_bias1, samp_bias2, cluster_info, cluster_num)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, seq1, seq2, sadj, sadj2, msk, samp_bias1, samp_bias2, cluster_info, cluster_num):\n\u001b[0;32m     18\u001b[0m     h_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mugcn(seq1, sadj, sadj2)  \u001b[39m#原feature\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     h_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mugcn(seq2, sadj, sadj2)  \u001b[39m# shuffled feature\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     22\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jz22185\\OneDrive - University of Bristol\\DCI-Fintech\\layers\\ugcn.py:26\u001b[0m, in \u001b[0;36mU_GCN.forward\u001b[1;34m(self, x, sadj, sadj2)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, sadj, sadj2):\n\u001b[0;32m     25\u001b[0m     emb1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSGAT1(x, sadj) \n\u001b[1;32m---> 26\u001b[0m     emb2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSGAT2(x, sadj2)\n\u001b[0;32m     27\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([emb1, emb2], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m     emb, att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(emb)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jz22185\\OneDrive - University of Bristol\\DCI-Fintech\\layers\\gat.py:21\u001b[0m, in \u001b[0;36mGAT.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj):\n\u001b[0;32m     20\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m---> 21\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([att(x, adj) \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m     24\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_att(x, adj))\n",
      "File \u001b[1;32mc:\\Users\\jz22185\\OneDrive - University of Bristol\\DCI-Fintech\\layers\\gat.py:21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj):\n\u001b[0;32m     20\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m---> 21\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([att(x, adj) \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m     24\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_att(x, adj))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jz22185\\OneDrive - University of Bristol\\DCI-Fintech\\layers\\graphattention.py:40\u001b[0m, in \u001b[0;36mGraphAttention.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     37\u001b[0m f_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(h, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma2)\n\u001b[0;32m     38\u001b[0m e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleakyrelu(f_1 \u001b[39m+\u001b[39m f_2\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m---> 40\u001b[0m zero_vec \u001b[39m=\u001b[39m \u001b[39m-\u001b[39;49m\u001b[39m9e15\u001b[39;49m\u001b[39m*\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(e)\n\u001b[0;32m     41\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(adj\u001b[39m.\u001b[39mto_dense() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, e, zero_vec)\n\u001b[0;32m     42\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attention, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "sig = torch.nn.Sigmoid()\n",
    "\n",
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def finetune(args, model_pretrain, device, test_graph, feats_num):\n",
    "    model = Classifier(args.num_layers, args.num_mlp_layers, feats_num, args.hidden_dim, args.final_dropout, args.neighbor_pooling_type, device).to(device)\n",
    "    \n",
    "    # replace the encoder in joint model with the pre-trained encoder，把外面训练好的CDI导进来，这个视角encoder吗\n",
    "    pretrained_dict = model_pretrain.state_dict()   # state_dict()是pytorch里调用所有参数信息的函数，这里是把DCI的参数信息存起来\n",
    "    model_dict = model.state_dict() #这里就是吧Classifier的参数信息存起来\n",
    "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}    # 只留下Classifier里有的字段的DCI的参数信息，为了防止下一步错叭\n",
    "    model_dict.update(pretrained_dict)  # 这两步就是把Classifier的参数信息更新成DCI的参数信息\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)  # 优化器的意思应该是反向传播的那个，不要纠结细节上次学过了虽然我知道你忘了\n",
    "    \n",
    "    criterion_tune = nn.BCEWithLogitsLoss() # 二分类的交叉熵损失函数\n",
    "\n",
    "    res = []\n",
    "    train_idx = test_graph[2]\n",
    "    node_train = test_graph[-1][train_idx, 0].astype('int')\n",
    "    label_train = torch.FloatTensor(test_graph[-1][train_idx, 1]).to(device)\n",
    "    for _ in range(1, args.finetune_epochs+1):  # 这一段应该就是在训练\n",
    "        model.train()\n",
    "        output = model(test_graph[0], test_graph[1])\n",
    "        loss = criterion_tune(output[node_train], torch.reshape(label_train, (-1, 1)))\n",
    "        \n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # testing\n",
    "        model.eval()    # 设成eval模式\n",
    "        auc = evaluate(model, test_graph)\n",
    "        res.append(auc)\n",
    "\n",
    "    return np.max(res)  # 返回最大的auc\n",
    "\n",
    "def evaluate(model, test_graph):\n",
    "    output = model(test_graph[0], test_graph[1])    #预测结果，其中0和1分别是adj和features，顺序记得确认一下\n",
    "    pred = sig(output.detach().cpu())   # 用激活函数压缩一下然后结果传到cpu上\n",
    "    test_idx = test_graph[3]    # 这个idx具体指什么要看model，我猜是abnormal data的idx\n",
    "    \n",
    "    labels = test_graph[-1] # 那么labels我猜就是abnormal data的label\n",
    "    pred = pred[labels[test_idx, 0].astype('int')].numpy()  # 不知道了这里回头再来看吧\n",
    "    target = labels[test_idx, 1]\n",
    "    \n",
    "    false_positive_rate, true_positive_rate, _ = metrics.roc_curve(target, pred, pos_label=1)\n",
    "    auc = metrics.auc(false_positive_rate, true_positive_rate)  # auc越接近1越好\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def main():\n",
    "parser = argparse.ArgumentParser(description='PyTorch deep cluster infomax')\n",
    "parser.add_argument('--dataset', type=str, default=\"wiki\",\n",
    "                    help='name of dataset (default: wiki)')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                    help='which gpu to use if any (default: 0)')\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='number of epochs to train (default: 50)')\n",
    "parser.add_argument('--num_layers', type=int, default=2,\n",
    "                    help='number of layers (default: 2)')\n",
    "parser.add_argument('--num_mlp_layers', type=int, default=2,\n",
    "                    help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=16,\n",
    "                    help='number of hidden units (default: 16)')\n",
    "parser.add_argument('--finetune_epochs', type=int, default=100,\n",
    "                    help='number of finetune epochs (default: 100)')\n",
    "parser.add_argument('--num_folds', type=int, default=10,\n",
    "                    help='number of folds (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--num_cluster', type=int, default=2,\n",
    "                    help='number of clusters (default: 2)')\n",
    "parser.add_argument('--recluster_interval', type=int, default=20,   # 指重新聚类的间隔，这里是每train 20次就重新聚类一次\n",
    "                    help='the interval of reclustering (default: 20)')\n",
    "parser.add_argument('--final_dropout', type=float, default=0.5,     # 指定模型在最后一层有几个node会变成0，是用来防止过拟合的，介于0-1，e.g. 0.5就是一半的node会变成0\n",
    "                    help='final layer dropout (default: 0.5)')\n",
    "parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"], # 指的是对每个node，它的adj node的feature是做sum还是average\n",
    "                    help='Pooling for over neighboring nodes: sum or average')\n",
    "parser.add_argument('--training_scheme', type=str, default=\"decoupled\", choices=[\"decoupled\", \"joint\"], # 用decoupled还是joint，因为对比的是DCI和别的所以没有提供joint DGI，我猜的\n",
    "                    help='Training schemes: decoupled or joint')\n",
    "# parser.add_argument('--module', type=str, default='U-GCN', choices=['DCI', 'U-GCN'],\n",
    "#                     help='module to generate feature matrix: DCI or U-GCN')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "setup_seed(0)\n",
    "\n",
    "# device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")   # 听说这里可以改简单一点\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "edge_index, feats, split_idx, label, nb_nodes = load_data(args.dataset, args.num_folds) # 这里的feats是自动生成的，不用管\n",
    "input_dim = feats.shape[1]\n",
    "\n",
    "kmeans = KMeans(n_clusters=args.num_cluster, random_state=0).fit(feats)\n",
    "ss_label = kmeans.labels_\n",
    "cluster_info = [list(np.where(ss_label==i)[0]) for i in range(args.num_cluster)]\n",
    "\n",
    "idx = np.random.permutation(nb_nodes)\n",
    "shuf_feats = feats[idx, :]\n",
    "\n",
    "adj_1hop, adj_2hop = load_graph(torch.LongTensor(edge_index), nb_nodes, device)\n",
    "feats = torch.FloatTensor(feats).to(device)\n",
    "shuf_feats = torch.FloatTensor(shuf_feats).to(device)\n",
    "\n",
    "model_pretrain = U_DCI(args.num_layers, args.num_mlp_layers, input_dim, args.hidden_dim, args.neighbor_pooling_type, device).to(device)\n",
    "optimizer_train = optim.Adam(model_pretrain.parameters(), lr=args.lr)\n",
    "store_loss = []\n",
    "progress_bar = tqdm(total = args.epochs)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model_pretrain.train()\n",
    "    loss_pretrain = model_pretrain(feats, shuf_feats, adj_1hop, adj_2hop, None, None, None, cluster_info, args.num_cluster)\n",
    "    if optimizer_train is not None:\n",
    "        optimizer_train.zero_grad()\n",
    "        loss_pretrain.backward()         \n",
    "        optimizer_train.step()\n",
    "        store_loss.append(loss_pretrain.item())\n",
    "        progress_bar.set_description(f'Epoch: {epoch}/{args.epochs} | Current loss: {loss_pretrain.item():.3f}')\n",
    "        progress_bar.update()\n",
    "    else:\n",
    "        print(f'Optimizer is none. Current epoch: {epoch}')\n",
    "    # re-clustering\n",
    "    if epoch % args.recluster_interval == 0 and epoch < args.epochs:\n",
    "        model_pretrain.eval()\n",
    "        emb = model_pretrain.get_emb(feats, adj_1hop, adj_2hop)\n",
    "        kmeans = KMeans(n_clusters=args.num_cluster, random_state=0).fit(emb.detach().cpu().numpy())\n",
    "        ss_label = kmeans.labels_\n",
    "        cluster_info = [list(np.where(ss_label==i)[0]) for i in range(args.num_cluster)]\n",
    "\n",
    "print(f'Pre-training Down!')\n",
    "\n",
    "#fine-tuning process\n",
    "fold_idx = 1\n",
    "every_fold_auc = []\n",
    "for (train_idx, test_idx) in split_idx: # split_idx在load data的时候就生成了\n",
    "    test_graph = (feats, adj_1hop, train_idx, test_idx, label)\n",
    "    tmp_auc = finetune(args, model_pretrain, device, test_graph, input_dim)\n",
    "    every_fold_auc.append(tmp_auc)\n",
    "    print('AUC on the Fold'+str(fold_idx)+': ', tmp_auc)    # 会返回每种folder的auc，每个auc是模型优化后最好的auc（因为是max）\n",
    "    fold_idx += 1\n",
    "print('The averaged AUC score: ', np.mean(every_fold_auc))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
